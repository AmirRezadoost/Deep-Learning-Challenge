{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOneqyadj/BURzqOjKgYFM4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmirRezadoost/image-classification-competitions/blob/main/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86JUYMCyiMAL",
        "outputId": "cd8b8716-2811-4288-85a6-ff5f5809b4e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "dset_dir = \"/content/drive/MyDrive/aiunict-2023/\" \n",
        "os.listdir(dset_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_Dz4ahLiUs9",
        "outputId": "d9a735e8-7c1b-48a6-d93b-80b4cb64f935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['test.csv', 'submission.csv', 'train.csv', 'test', 'train']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqs9r8Rbi5vY",
        "outputId": "4b12bc4f-7ae5-404c-cd0d-a7bff977d1bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/519.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.1+cu118)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.11.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import os\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from torch.nn import functional as F\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.ops.boxes import box_iou\n",
        "from torchvision.transforms.functional import to_pil_image, to_tensor\n",
        "import torch.nn.functional as F\n",
        "from torchvision.transforms.functional import resize\n",
        "import torchvision\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from torchvision import transforms as torchtrans  \n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
        "from torch.utils.data import random_split\n",
        "from matplotlib import pyplot as plt\n",
        "from numpy import asarray\n",
        "import torchmetrics\n",
        "from torchmetrics.classification import Accuracy\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "dest_dir = \"\""
      ],
      "metadata": {
        "id": "Va2OsBmtiUzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UnictImg(Dataset):\n",
        "    def __init__(self, dset_dir, train=True,mask=False,res_bb_perc=0, transforms=None):\n",
        "        \n",
        "        self.split = \"train\" if train else \"test\"\n",
        "        self.dset_dir = Path(dset_dir)/self.split\n",
        "        self.transforms = transforms\n",
        "        self.res_bb_perc=res_bb_perc\n",
        "        self.mask=mask\n",
        "        self.files = []\n",
        "        self.df_bb=pd.DataFrame()\n",
        "        self.df_bb=pd.read_csv(dset_dir+\"train.csv\")\n",
        "        folders = sorted(os.listdir(self.dset_dir))\n",
        "        self.file=folders\n",
        "        if self.split==\"train\" :         \n",
        "            for folder in folders:\n",
        "                class_idx= folders.index(folder)\n",
        "                folder_dir = self.dset_dir/folder\n",
        "                files = os.listdir(folder_dir)\n",
        "                if(class_idx==0):\n",
        "                    self.files+=[{\"mask\":folder+\"/\"+x,\"file\": folder_dir/x, \"class\": class_idx+1,\"flag\":True} for x in files]\n",
        "                    if(mask):\n",
        "                        self.files+=[{\"mask\":folder+\"/\"+x,\"file\": folder_dir/x, \"class\": class_idx+1,\"flag\":True} for x in files]\n",
        "                else:\n",
        "                    self.files+=[{\"mask\":folder+\"/\"+x,\"file\": folder_dir/x, \"class\": class_idx+1,\"flag\":False} for x in files]\n",
        "                    if(mask):\n",
        "                        self.files+=[{\"mask\":folder+\"/\"+x,\"file\": folder_dir/x, \"class\": class_idx+1,\"flag\":False} for x in files]\n",
        "\n",
        "        else:\n",
        "            self.files+=[self.dset_dir/file for file in folders]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        \n",
        "        if self.split == \"train\":\n",
        "            \n",
        "            item = self.files[i]\n",
        "            file = item['file']\n",
        "            class_idx=item['class']\n",
        "            # reading the images and converting them to correct size and color    \n",
        "            img = cv2.imread(str(file))\n",
        "            img_res = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "            # diving by 255\n",
        "            img_res /= 255.0\n",
        "            \n",
        "            # recover bounding boxes\n",
        "            name_file_img=item['mask']\n",
        "            bb=self.df_bb[self.df_bb[\"image\"]==name_file_img]\n",
        "          \n",
        "            # resize bounding boxes\n",
        "            xmin=int(bb[\"x1\"])+1\n",
        "            ymin=int(bb[\"y1\"])+1\n",
        "            xmax=int(bb[\"x2\"])-1\n",
        "            ymax=int(bb[\"y2\"])-1\n",
        "            res_bb_perc=self.res_bb_perc\n",
        "            boxes = [[xmin+((xmax-xmin)*res_bb_perc), \n",
        "                      ymin+((ymax-ymin)*res_bb_perc), \n",
        "                      xmax-((xmax-xmin)*res_bb_perc), \n",
        "                      ymax-((ymax-ymin)*res_bb_perc)]]           \n",
        "           \n",
        "                    \n",
        "            labels = torch.tensor(item['class'],dtype=torch.int64)\n",
        "            labels=labels.unsqueeze(0)\n",
        "                                    \n",
        "            # applying transformations            \n",
        "            sample = {'image' : img_res,\n",
        "                      'bboxes' : boxes,\n",
        "                      'labels' : labels}\n",
        "               \n",
        "            sample = self.transforms(**sample)\n",
        "            target = {}\n",
        "            target[\"boxes\"] =torch.tensor(sample['bboxes'],dtype=torch.int64)\n",
        "            target[\"labels\"] = torch.tensor(sample['labels'],dtype=torch.int64)\n",
        "            img_res = sample['image']\n",
        "            \n",
        "            if(self.mask and item['flag']):\n",
        "                \n",
        "                bb=target['boxes'][0]\n",
        "                #creating masked image\n",
        "                mask=torch.rand(3,224,224)\n",
        "                mask[0:3,bb[1]:bb[3],bb[0]:bb[2]]= img_res[0:3,bb[1]:bb[3],bb[0]:bb[2]]\n",
        "                img_res=mask\n",
        "                \n",
        "            if(self.mask):\n",
        "                target[\"labels\"] =  target[\"labels\"]-1\n",
        "                class_idx-=1\n",
        "               \n",
        "            return img_res, target,class_idx\n",
        "        else:\n",
        "            file = self.files[i]\n",
        "            # reading the images and converting them to correct size and color    \n",
        "            img = cv2.imread(str(file))\n",
        "            img_res = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "            # diving by 255\n",
        "            img_res /= 255.0\n",
        "            \n",
        "            sample = {'image': img_res}\n",
        "            sample = self.transforms(**sample)\n",
        "            image = sample['image']\n",
        "           \n",
        "            return image,self.file[i]\n",
        "        \n",
        "\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "ryHTKP17i-Hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Albumentations\n",
        "def get_train_transform():\n",
        "    return A.Compose([\n",
        "        A.Flip(1),\n",
        "        A.Resize(224,224),\n",
        "        A.ShiftScaleRotate(shift_limit=0.2,scale_limit=0.2, rotate_limit=45, p=0.5),\n",
        "        ToTensorV2(p=1.0)\n",
        "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "\n",
        "def get_test_transform():\n",
        "    return A.Compose([\n",
        "        #A.Normalize(mean=(0, 0, 0), std=(1, 1, 1), max_pixel_value=255.0, p=1.0),\n",
        "        A.Resize(224,224),\n",
        "        ToTensorV2(p=1.0)\n",
        "    ])"
      ],
      "metadata": {
        "id": "cw9Lp0ktjBSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the data loader\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "train_dataset = UnictImg(dset_dir, train=True,res_bb_perc=0.1,transforms= get_train_transform())\n",
        "test_dataset = UnictImg(dset_dir, train=False, transforms= get_test_transform())\n",
        "train_data_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
        "test_data_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "qJor7by-jDlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "num_classes = 9 # 8 Classes + 1 background\n",
        "# get number of input features for the classifier\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "# replace the pre-trained head with a new one\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
      ],
      "metadata": {
        "id": "NX9I-2qIjE1s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10aef2bb-5df1-4e42-80b4-8d1c310fd4c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
            "100%|██████████| 160M/160M [00:00<00:00, 240MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "metadata": {
        "id": "Qjd1oam7jGvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the optimizer\n",
        "import torch.optim as optim\n",
        "# Define the optimizer and learning rate scheduler\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0001)\n",
        "lr_scheduler =torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.2)"
      ],
      "metadata": {
        "id": "35MdS7ucjIju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Averager:\n",
        "    def __init__(self):\n",
        "        self.current_total = 0.0\n",
        "        self.iterations = 0.0\n",
        "\n",
        "    def send(self, value):\n",
        "        self.current_total += value\n",
        "        self.iterations += 1\n",
        "\n",
        "    @property\n",
        "    def value(self):\n",
        "        if self.iterations == 0:\n",
        "            return 0\n",
        "        else:\n",
        "            return 1.0 * self.current_total / self.iterations\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_total = 0.0\n",
        "        self.iterations = 0.0"
      ],
      "metadata": {
        "id": "kq8Ac2kCjJzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_nms(orig_prediction, iou_thresh=0.3):\n",
        "    \n",
        "    # torchvision returns the indices of the bboxes to keep\n",
        "    keep = torchvision.ops.nms(orig_prediction['boxes'], orig_prediction['scores'], iou_thresh)\n",
        "    final_prediction = orig_prediction\n",
        "    final_prediction['boxes'] = final_prediction['boxes'][keep]\n",
        "    final_prediction['scores'] = final_prediction['scores'][keep]\n",
        "    final_prediction['labels'] = final_prediction['labels'][keep]\n",
        "    \n",
        "    return final_prediction"
      ],
      "metadata": {
        "id": "fZtsufYpjLnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "id": "QTi2NPkdjNGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 7\n",
        "loss_hist = Averager()\n",
        "itr = 1\n",
        "loss_history_iter = []\n",
        "loss_history_epoch = []\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    loss_hist.reset()\n",
        "    model.train()\n",
        "    for images, targets in train_data_loader:\n",
        "        images = [img.to(device) for img in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "\n",
        "        # Forward pass with activations\n",
        "        loss_dict = model(images, targets) \n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        loss_value = losses.item()\n",
        "        loss_hist.send(loss_value)\n",
        "        loss_history_iter.append(loss_value)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if itr % 50 == 0:\n",
        "            print(f\"Iteration #{itr} loss: {loss_value}\")\n",
        "        itr += 1\n",
        "\n",
        "    # update the learning rate\n",
        "    if lr_scheduler is not None:\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    loss_history_epoch.append(loss_hist.value)\n",
        "    print(f\"Epoch #{epoch} loss: {loss_hist.value}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIx_LaNtjRa7",
        "outputId": "2cf3389f-bd03-42ce-b394-73b0eb3df92c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration #50 loss: 0.2901206910610199\n",
            "Iteration #100 loss: 0.33414873480796814\n",
            "Epoch #0 loss: 0.3326826077699661\n",
            "Iteration #150 loss: 0.2596089541912079\n",
            "Iteration #200 loss: 0.2297118753194809\n",
            "Epoch #1 loss: 0.26098056599497793\n",
            "Iteration #250 loss: 0.18378813564777374\n",
            "Iteration #300 loss: 0.14045818150043488\n",
            "Epoch #2 loss: 0.1755467890202999\n",
            "Iteration #350 loss: 0.12668585777282715\n",
            "Iteration #400 loss: 0.12300259619951248\n",
            "Epoch #3 loss: 0.139712822586298\n",
            "Iteration #450 loss: 0.12473265081644058\n",
            "Iteration #500 loss: 0.1313989758491516\n",
            "Epoch #4 loss: 0.12864135317504405\n",
            "Iteration #550 loss: 0.10489065945148468\n",
            "Iteration #600 loss: 0.12398514151573181\n",
            "Epoch #5 loss: 0.12454748265445233\n",
            "Iteration #650 loss: 0.14879648387432098\n",
            "Iteration #700 loss: 0.09722486883401871\n",
            "Epoch #6 loss: 0.1197850426286459\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_list=[]\n",
        "class_list=[]\n",
        "for idx in range(test_dataset.__len__()):\n",
        "    img,name_file = test_dataset[idx]\n",
        "\n",
        "    # put the model in evaluation mode\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        prediction = model([img.to(device)])[0]\n",
        "    nms_prediction = apply_nms(prediction, iou_thresh=0.5)\n",
        "    pred=nms_prediction['labels'].cpu().numpy()[0]\n",
        "    \n",
        "    img_list.append(name_file)\n",
        "    class_list.append(pred-1)\n",
        "    #print(\"file: \"+str(file)+\" preds: \"+str(preds))\n",
        "    \n",
        "import pandas as pd\n",
        "\n",
        "d = {'image': img_list, 'class': class_list}\n",
        "df = pd.DataFrame(data=d)\n",
        "df.to_csv(\"Submission.csv\",index=False)"
      ],
      "metadata": {
        "id": "0LoBJzaujVkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "yPMGO6S9cuIJ",
        "outputId": "f526eb27-78de-470f-dd6b-f9549a875d10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     image  class\n",
              "0  000.jpg      3\n",
              "1  001.jpg      0\n",
              "2  002.jpg      3\n",
              "3  003.jpg      4\n",
              "4  004.jpg      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f9f92a76-3b72-4bc6-b5f1-381747e098ed\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>000.jpg</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>001.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>002.jpg</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>003.jpg</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>004.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f9f92a76-3b72-4bc6-b5f1-381747e098ed')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f9f92a76-3b72-4bc6-b5f1-381747e098ed button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f9f92a76-3b72-4bc6-b5f1-381747e098ed');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}